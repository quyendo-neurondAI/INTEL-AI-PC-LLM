{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openvino_genai import LLMPipeline, GenerationConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_DIR = r\"model-path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = LLMPipeline(MODEL_DIR, device=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GenerationConfig(\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    apply_chat_template=False,   # <-- as you set\n",
    ")\n",
    "\n",
    "prompt = \"Give me 5 quick facts about eigenvalues.\"\n",
    "\n",
    "# Load tokenizer from the same model folder to count tokens accurately\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- streaming, but chunks are characters ---\n",
    "pieces = []\n",
    "t_start = time.perf_counter()\n",
    "t_first = None\n",
    "\n",
    "for chunk in pipe.generate(prompt, gen):\n",
    "    if t_first is None:\n",
    "        t_first = time.perf_counter()   # time to first *character* received\n",
    "    # Chunk may be a tiny object or just a char; normalize to a string:\n",
    "    s = getattr(chunk, \"token\", getattr(chunk, \"text\", str(chunk)))\n",
    "    pieces.append(s)\n",
    "\n",
    "t_end = time.perf_counter()\n",
    "decoded_text = \"\".join(pieces)\n",
    "\n",
    "# --- true token accounting (model tokens, not characters) ---\n",
    "prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "full_ids   = tokenizer(prompt + decoded_text, add_special_tokens=False).input_ids\n",
    "\n",
    "prompt_tokens = len(prompt_ids)\n",
    "new_tokens    = len(full_ids) - prompt_tokens\n",
    "\n",
    "# --- timings / throughput ---\n",
    "total_time   = t_end - t_start\n",
    "decode_time  = (t_end - t_first) if t_first else total_time  # time after first char arrived\n",
    "ttft         = (t_first - t_start) if t_first else None\n",
    "\n",
    "tps_total  = (new_tokens / total_time)  if total_time  > 0 else float(\"inf\")\n",
    "tps_decode = (new_tokens / decode_time) if decode_time > 0 else float(\"inf\")\n",
    "\n",
    "print(decoded_text)\n",
    "print(\"\\n--- Stats ---\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"New tokens:    {new_tokens}\")\n",
    "\n",
    "print(f\"Total time:     {total_time:.3f}s   | Tokens/sec (total):  {tps_total:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
